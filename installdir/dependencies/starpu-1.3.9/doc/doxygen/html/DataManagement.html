<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>StarPU Handbook: Data Management</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">StarPU Handbook
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('DataManagement.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Data Management </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>TODO: intro which mentions consistency among other things</p>
<h1><a class="anchor" id="DataInterface"></a>
Data Interface</h1>
<p>StarPU provides several data interfaces for programmers to describe the data layout of their application. There are predefined interfaces already available in StarPU. Users can define new data interfaces as explained in <a class="el" href="DataManagement.html#DefiningANewDataInterface">Defining A New Data Interface</a>. All functions provided by StarPU are documented in <a class="el" href="group__API__Data__Interfaces.html">Data Interfaces</a>. You will find a short list below.</p>
<h2><a class="anchor" id="VariableDataInterface"></a>
Variable Data Interface</h2>
<p>A variable is a given-size byte element, typically a scalar. Here an example of how to register a variable data to StarPU by using <a class="el" href="group__API__Data__Interfaces.html#gafb92c2538dc629d823221c5dc16bd767">starpu_variable_data_register()</a>.</p>
<div class="fragment"><div class="line"><span class="keywordtype">float</span> var = 42.0;</div><div class="line"><a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> var_handle;</div><div class="line"><a class="code" href="group__API__Data__Interfaces.html#gafb92c2538dc629d823221c5dc16bd767">starpu_variable_data_register</a>(&amp;var_handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, (uintptr_t)&amp;var, <span class="keyword">sizeof</span>(var));</div></div><!-- fragment --><h2><a class="anchor" id="VectorDataInterface"></a>
Vector Data Interface</h2>
<p>A vector is a fixed number of elements of a given size. Here an example of how to register a vector data to StarPU by using <a class="el" href="group__API__Data__Interfaces.html#ga4248716bc322e1628b86365d7b9a8822">starpu_vector_data_register()</a>.</p>
<div class="fragment"><div class="line"><span class="keywordtype">float</span> vector[NX];</div><div class="line"><a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> vector_handle;</div><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga4248716bc322e1628b86365d7b9a8822">starpu_vector_data_register</a>(&amp;vector_handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, (uintptr_t)vector, NX, <span class="keyword">sizeof</span>(vector[0]));</div></div><!-- fragment --><p>Vectors can be partitioned into pieces by using <a class="el" href="group__API__Data__Partition.html#ga212189d3b83dfa4e225609b5f2bf8461">starpu_vector_filter_block()</a>. They can also be partitioned with some overlapping by using <a class="el" href="group__API__Data__Partition.html#gaab49915dc0462c1b145bfb0a9ce4cf52">starpu_vector_filter_block_shadow()</a>. By default StarPU uses the same size for each piece. If different sizes are desired, <a class="el" href="group__API__Data__Partition.html#gab9fa487bfff5ccdd59210bdde65a11db">starpu_vector_filter_list()</a> or <a class="el" href="group__API__Data__Partition.html#ga6c3d612b5161b5f72282b4bf9390525a">starpu_vector_filter_list_long()</a> can be used instead. To just divide in two pieces, <a class="el" href="group__API__Data__Partition.html#gab639622ea4929c36df704a0bebfd3fac">starpu_vector_filter_divide_in_2()</a> can be used.</p>
<h2><a class="anchor" id="MatrixDataInterface"></a>
Matrix Data Interface</h2>
<p>To register 2-D matrices with a potential padding, one can use the matrix data interface. Here an example of how to register a matrix data to StarPU by using <a class="el" href="group__API__Data__Interfaces.html#ga833b734aa76cdee89245cb0710793cf9">starpu_matrix_data_register()</a>.</p>
<div class="fragment"><div class="line"><span class="keywordtype">float</span> *matrix;</div><div class="line"><a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> matrix_handle;</div><div class="line">matrix = (<span class="keywordtype">float</span>*)malloc(width * height * <span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga833b734aa76cdee89245cb0710793cf9">starpu_matrix_data_register</a>(&amp;matrix_handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, (uintptr_t)matrix, width, width, height, <span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div></div><!-- fragment --><p>2D matrices can be partitioned into 2D matrices along the x dimension by using <a class="el" href="group__API__Data__Partition.html#ga8c86b2af9e0806e631c1cbb5d506506b">starpu_matrix_filter_block()</a>, and along the y dimension by using <a class="el" href="group__API__Data__Partition.html#ga2925be576ac7d597ecead381ff32a894">starpu_matrix_filter_vertical_block()</a>. They can also be partitioned with some overlapping by using <a class="el" href="group__API__Data__Partition.html#ga88fbca61843b76314e39a2c0f8b93d6c">starpu_matrix_filter_block_shadow()</a> and <a class="el" href="group__API__Data__Partition.html#ga7132923bd901e0e4254cc0b20d49997a">starpu_matrix_filter_vertical_block_shadow()</a>.</p>
<h2><a class="anchor" id="BlockDataInterface"></a>
Block Data Interface</h2>
<p>To register 3-D matrices with potential paddings on Y and Z dimensions, one can use the block data interface. Here an example of how to register a block data to StarPU by using <a class="el" href="group__API__Data__Interfaces.html#gaa093608060a222a89fff01130e9c5c64">starpu_block_data_register()</a>.</p>
<div class="fragment"><div class="line"><span class="keywordtype">float</span> *block;</div><div class="line"><a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> block_handle;</div><div class="line">block = (<span class="keywordtype">float</span>*)malloc(nx*ny*nz*<span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div><div class="line"><a class="code" href="group__API__Data__Interfaces.html#gaa093608060a222a89fff01130e9c5c64">starpu_block_data_register</a>(&amp;block_handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, (uintptr_t)block, nx, nx*ny, nx, ny, nz, <span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div></div><!-- fragment --><p>3D matrices can be partitioned along the x dimension by using <a class="el" href="group__API__Data__Partition.html#ga1a265ffca51fae58701832a4daa53bd9">starpu_block_filter_block()</a>, or along the y dimension by using starpu_block_filter_vertical_block, or along the z dimension by using starpu_block_filter_depth_block. They can also be partitioned with some overlapping by using <a class="el" href="group__API__Data__Partition.html#ga7cc8832e25f2f4049ba5a0053b122dd9">starpu_block_filter_block_shadow()</a>, <a class="el" href="group__API__Data__Partition.html#gafa4818b571e98acd8696a1251b0d4e74">starpu_block_filter_vertical_block_shadow()</a>, or <a class="el" href="group__API__Data__Partition.html#gac4b9ec529f67e5c300e7eed3e185fbaf">starpu_block_filter_depth_block_shadow()</a>.</p>
<h2><a class="anchor" id="BCSRDataInterface"></a>
BCSR Data Interface</h2>
<p>BCSR (Blocked Compressed Sparse Row Representation) sparse matrix data can be registered to StarPU using the bcsr data interface. Here an example on how to do so by using <a class="el" href="group__API__Data__Interfaces.html#ga30da5dedded6584f211cb41d6811bd2e">starpu_bcsr_data_register()</a>.</p>
<div class="fragment"><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * We use the following matrix:</span></div><div class="line"><span class="comment">  </span></div><div class="line"><span class="comment"> *   +----------------+</span></div><div class="line"><span class="comment"> *   |  0   1   0   0 |</span></div><div class="line"><span class="comment"> *   |  2   3   0   0 |</span></div><div class="line"><span class="comment"> *   |  4   5   8   9 |</span></div><div class="line"><span class="comment"> *   |  6   7  10  11 |</span></div><div class="line"><span class="comment"> *   +----------------+</span></div><div class="line"><span class="comment">  </span></div><div class="line"><span class="comment"> * nzval  = [0, 1, 2, 3] ++ [4, 5, 6, 7] ++ [8, 9, 10, 11]</span></div><div class="line"><span class="comment"> * colind = [0, 0, 1]</span></div><div class="line"><span class="comment"> * rowptr = [0, 1, 3]</span></div><div class="line"><span class="comment"> * r = c = 2</span></div><div class="line"><span class="comment"> */</span></div><div class="line"></div><div class="line"><span class="comment">/* Size of the blocks */</span></div><div class="line"><span class="keywordtype">int</span> R = 2;</div><div class="line"><span class="keywordtype">int</span> C = 2;</div><div class="line"></div><div class="line"><span class="keywordtype">int</span> NROWS = 2;</div><div class="line"><span class="keywordtype">int</span> NNZ_BLOCKS = 3;    <span class="comment">/* out of 4 */</span></div><div class="line"><span class="keywordtype">int</span> NZVAL_SIZE = (R*C*NNZ_BLOCKS);</div><div class="line"></div><div class="line"><span class="keywordtype">int</span> nzval[NZVAL_SIZE]  =</div><div class="line">{</div><div class="line">        0, 1, 2, 3,    <span class="comment">/* First block  */</span></div><div class="line">        4, 5, 6, 7,    <span class="comment">/* Second block */</span></div><div class="line">        8, 9, 10, 11   <span class="comment">/* Third block  */</span></div><div class="line">};</div><div class="line">uint32_t colind[NNZ_BLOCKS] =</div><div class="line">{</div><div class="line">        0, <span class="comment">/* block-column index for first block in nzval */</span></div><div class="line">        0, <span class="comment">/* block-column index for second block in nzval */</span></div><div class="line">        1  <span class="comment">/* block-column index for third block in nzval */</span></div><div class="line">};</div><div class="line">uint32_t rowptr[NROWS+1] =</div><div class="line">{</div><div class="line">        0, / * block-index in nzval of the first block of the first row. */</div><div class="line">        1, / * block-index in nzval of the first block of the second row. */</div><div class="line">        NNZ_BLOCKS <span class="comment">/* number of blocks, to allow an easier element&#39;s access for the kernels */</span></div><div class="line">};</div><div class="line"></div><div class="line"><a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> bcsr_handle;</div><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga30da5dedded6584f211cb41d6811bd2e">starpu_bcsr_data_register</a>(&amp;bcsr_handle,</div><div class="line">                          <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>,</div><div class="line">                          NNZ_BLOCKS,</div><div class="line">                          NROWS,</div><div class="line">                          (uintptr_t) nzval,</div><div class="line">                          colind,</div><div class="line">                          rowptr,</div><div class="line">                          0, <span class="comment">/* firstentry */</span></div><div class="line">                          R,</div><div class="line">                          C,</div><div class="line">                          <span class="keyword">sizeof</span>(nzval[0]));</div></div><!-- fragment --><p>StarPU provides an example on how to deal with such matrices in <code>examples/spmv</code>.</p>
<p>BCSR data handles can be partitioned into its dense matrix blocks by using <a class="el" href="group__API__Data__Partition.html#ga0e1bee4821237529d554605d333e9109">starpu_bcsr_filter_canonical_block()</a>, or split into other BCSR data handles by using <a class="el" href="group__API__Data__Partition.html#ga4b910784c11bb46599fe9cb8928418f2">starpu_bcsr_filter_vertical_block()</a> (but only split along the leading dimension is supported, i.e. along adjacent nnz blocks)</p>
<h2><a class="anchor" id="CSRDataInterface"></a>
CSR Data Interface</h2>
<p>TODO</p>
<p>CSR data handles can be partitioned into vertical CSR matrices by using <a class="el" href="group__API__Data__Partition.html#ga554a2fb14fdee9353364c39f36ee3a6f">starpu_csr_filter_vertical_block()</a>.</p>
<h2><a class="anchor" id="VariableSizeDataInterface"></a>
Data Interface with Variable Size</h2>
<p>Tasks are actually allowed to change the size of data interfaces.</p>
<p>The simplest case is just changing the amount of data actually used within the allocated buffer. This is for instance implemented for the matrix interface: one can set the new NX/NY values with <a class="el" href="group__API__Data__Interfaces.html#ga6f64340041ca2277f071aebd9bd3ea7b">STARPU_MATRIX_SET_NX()</a>, <a class="el" href="group__API__Data__Interfaces.html#ga95f3f5141265f8a78f8b5802be7ea20c">STARPU_MATRIX_SET_NY()</a>, and <a class="el" href="group__API__Data__Interfaces.html#ga91461bface98435f7e90d515c4aa04cb">STARPU_MATRIX_SET_LD()</a> at the end of the task implementation. Data transfers achieved by StarPU will then use these values instead of the whole allocated size. The values of course need to be set within the original allocation. To reserve room for increasing the NX/NY values, one can use <a class="el" href="group__API__Data__Interfaces.html#ga341ae5d71140a231be9dfab9d65dd116">starpu_matrix_data_register_allocsize()</a> instead of <a class="el" href="group__API__Data__Interfaces.html#ga833b734aa76cdee89245cb0710793cf9">starpu_matrix_data_register()</a>, to specify the allocation size to be used instead of the default NX*NY*ELEMSIZE. To support this, the data interface has to implement the <a class="el" href="group__API__Data__Interfaces.html#abc69fa030b4398af1663af656d91c2c5">starpu_data_interface_ops::alloc_footprint</a> and <a class="el" href="group__API__Data__Interfaces.html#a223bf2a2e0fe3a2c8042bdb90605c24d">starpu_data_interface_ops::alloc_compare</a> methods, for proper StarPU allocation management.</p>
<p>A more involved case is changing the amount of allocated data. The task implementation can just reallocate the buffer during its execution, and set the proper new values in the interface structure, e.g. nx, ny, ld, etc. so that the StarPU core knows the new data layout. The <a class="el" href="group__API__Data__Interfaces.html#structstarpu__data__interface__ops">starpu_data_interface_ops</a> structure however then needs to have the <a class="el" href="group__API__Data__Interfaces.html#a2c4333ac3a911e749197069fadca1568">starpu_data_interface_ops::dontcache</a> field set to 1, to prevent StarPU from trying to perform any cached allocation, since the allocated size will vary. An example is available in <code>tests/datawizard/variable_size.c</code>. The example uses its own data interface so as to contain some simulation information for data growth, but the principle can be applied for any data interface.</p>
<p>The principle is to use <code>starpu_malloc_on_node_flags</code> to make the new allocation, and use <code>starpu_free_on_node_flags</code> to release any previous allocation. The flags have to be precisely like in the example:</p>
<div class="fragment"><div class="line"><span class="keywordtype">unsigned</span> workerid = <a class="code" href="group__API__Workers__Properties.html#ga177bffe36c7df3f91aaebe2dd03b8cda">starpu_worker_get_id_check</a>();</div><div class="line"><span class="keywordtype">unsigned</span> dst_node = <a class="code" href="group__API__Workers__Properties.html#ga7de6654141ce89ea83c3aba60486396e">starpu_worker_get_memory_node</a>(workerid);</div><div class="line">interface-&gt;ptr = <a class="code" href="group__API__Data__Interfaces.html#gab2bf7713cad5570775bdf4efec79502d">starpu_malloc_on_node_flags</a>(dst_node, size + increase, <a class="code" href="group__API__Standard__Memory__Library.html#ga2abe959e39acfb75c7c0652706dfe84c">STARPU_MALLOC_PINNED</a> | <a class="code" href="group__API__Standard__Memory__Library.html#gaace5eebbb6662fb1be7c79a65464e1cc">STARPU_MALLOC_COUNT</a> | <a class="code" href="group__API__Standard__Memory__Library.html#gaffa4b2a7af68027551855bd6c9520914">STARPU_MEMORY_OVERFLOW</a>);</div><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga02005aa2a3c838802d95d0426a937d8d">starpu_free_on_node_flags</a>(dst_node, old, size, <a class="code" href="group__API__Standard__Memory__Library.html#ga2abe959e39acfb75c7c0652706dfe84c">STARPU_MALLOC_PINNED</a> | <a class="code" href="group__API__Standard__Memory__Library.html#gaace5eebbb6662fb1be7c79a65464e1cc">STARPU_MALLOC_COUNT</a> | <a class="code" href="group__API__Standard__Memory__Library.html#gaffa4b2a7af68027551855bd6c9520914">STARPU_MEMORY_OVERFLOW</a>);</div><div class="line">interface-&gt;size += increase;</div></div><!-- fragment --><p>so that the allocated area has the expected properties and the allocation is accounted for properly.</p>
<p>Depending on the interface (vector, CSR, etc.) you may have to fix several members of the data interface: e.g. both <code>nx</code> and <code>allocsize</code> for vectors, and store the pointer both in <code>ptr</code> and <code>dev_handle</code>.</p>
<p>Some interfaces make a distinction between the actual number of elements stored in the data and the actually allocated buffer. For instance, the vector interface uses the <code>nx</code> field for the former, and the <code>allocsize</code> for the latter. This allows for lazy reallocation to avoid reallocating the buffer everytime to exactly match the actual number of elements. Computations and data transfers will use <code>nx</code> field, while allocation functions will use the <code>allocsize</code>. One just has to make sure that <code>allocsize</code> is always bigger or equal to <code>nx</code>.</p>
<p>Important note: one can not change the size of a partitioned data.</p>
<h1><a class="anchor" id="DataManagement"></a>
Data Management</h1>
<p>When the application allocates data, whenever possible it should use the <a class="el" href="group__API__Standard__Memory__Library.html#ga49603eaea3b05e8ced9ba1bd873070c3">starpu_malloc()</a> function, which will ask CUDA or OpenCL to make the allocation itself and pin the corresponding allocated memory, or to use the <a class="el" href="group__API__Standard__Memory__Library.html#ga5a6ea6d03d7b0f4a97a8046b30ecd0bb">starpu_memory_pin()</a> function to pin memory allocated by other ways, such as local arrays. This is needed to permit asynchronous data transfer, i.e. permit data transfer to overlap with computations. Otherwise, the trace will show that the <code>DriverCopyAsync</code> state takes a lot of time, this is because CUDA or OpenCL then reverts to synchronous transfers.</p>
<p>The application can provide its own allocation function by calling <a class="el" href="group__API__Standard__Memory__Library.html#ga50b58997f2ab30eb82baab2cc7893a08">starpu_malloc_set_hooks()</a>. StarPU will then use them for all data handle allocations in the main memory.</p>
<p>By default, StarPU leaves replicates of data wherever they were used, in case they will be re-used by other tasks, thus saving the data transfer time. When some task modifies some data, all the other replicates are invalidated, and only the processing unit which ran this task will have a valid replicate of the data. If the application knows that this data will not be re-used by further tasks, it should advise StarPU to immediately replicate it to a desired list of memory nodes (given through a bitmask). This can be understood like the write-through mode of CPU caches.</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Management.html#gae53e7b21c7426c9845a1046cfe5becce">starpu_data_set_wt_mask</a>(img_handle, 1&lt;&lt;0);</div></div><!-- fragment --><p>will for instance request to always automatically transfer a replicate into the main memory (node <code>0</code>), as bit <code>0</code> of the write-through bitmask is being set.</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Management.html#gae53e7b21c7426c9845a1046cfe5becce">starpu_data_set_wt_mask</a>(img_handle, ~0U);</div></div><!-- fragment --><p>will request to always automatically broadcast the updated data to all memory nodes.</p>
<p>Setting the write-through mask to <code>~0U</code> can also be useful to make sure all memory nodes always have a copy of the data, so that it is never evicted when memory gets scarse.</p>
<p>Implicit data dependency computation can become expensive if a lot of tasks access the same piece of data. If no dependency is required on some piece of data (e.g. because it is only accessed in read-only mode, or because write accesses are actually commutative), use the function <a class="el" href="group__API__Data__Management.html#ga273df19e4cad1c05ec5df697bcec4444">starpu_data_set_sequential_consistency_flag()</a> to disable implicit dependencies on this data.</p>
<p>In the same vein, accumulation of results in the same data can become a bottleneck. The use of the mode <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaa92c1e935fb451cc339a21976680ee05">STARPU_REDUX</a> permits to optimize such accumulation (see <a class="el" href="DataManagement.html#DataReduction">Data Reduction</a>). To a lesser extent, the use of the flag <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbade64878c777f3655d8802cbc6b48a95d">STARPU_COMMUTE</a> keeps the bottleneck (see <a class="el" href="DataManagement.html#DataCommute">Commute Data Access</a>), but at least permits the accumulation to happen in any order.</p>
<p>Applications often need a data just for temporary results. In such a case, registration can be made without an initial value, for instance this produces a vector data:</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga4248716bc322e1628b86365d7b9a8822">starpu_vector_data_register</a>(&amp;handle, -1, 0, n, <span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div></div><!-- fragment --><p>StarPU will then allocate the actual buffer only when it is actually needed, e.g. directly on the GPU without allocating in main memory.</p>
<p>In the same vein, once the temporary results are not useful any more, the data should be thrown away. If the handle is not to be reused, it can be unregistered:</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Management.html#ga4fa34753bff1d29c20f0a0e361020b4e">starpu_data_unregister_submit</a>(handle);</div></div><!-- fragment --><p>actual unregistration will be done after all tasks working on the handle terminate.</p>
<p>If the handle is to be reused, instead of unregistering it, it can simply be invalidated:</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Management.html#ga06b01fdf769f8f2eb222ecde42afbc81">starpu_data_invalidate_submit</a>(handle);</div></div><!-- fragment --><p>the buffers containing the current value will then be freed, and reallocated only when another task writes some value to the handle.</p>
<h1><a class="anchor" id="DataPrefetch"></a>
Data Prefetch</h1>
<p>The scheduling policies <code>heft</code>, <code>dmda</code> and <code>pheft</code> perform data prefetch (see <a class="el" href="ExecutionConfigurationThroughEnvironmentVariables.html#STARPU_PREFETCH">STARPU_PREFETCH</a>): as soon as a scheduling decision is taken for a task, requests are issued to transfer its required data to the target processing unit, if needed, so that when the processing unit actually starts the task, its data will hopefully be already available and it will not have to wait for the transfer to finish.</p>
<p>The application may want to perform some manual prefetching, for several reasons such as excluding initial data transfers from performance measurements, or setting up an initial statically-computed data distribution on the machine before submitting tasks, which will thus guide StarPU toward an initial task distribution (since StarPU will try to avoid further transfers).</p>
<p>This can be achieved by giving the function <a class="el" href="group__API__Data__Management.html#ga57687b811ced00dbfc35af73164a72aa">starpu_data_prefetch_on_node()</a> the handle and the desired target memory node. The <a class="el" href="group__API__Data__Management.html#gad5a24f94d0aa2bbfab8957c3dd13949a">starpu_data_idle_prefetch_on_node()</a> variant can be used to issue the transfer only when the bus is idle.</p>
<p>Conversely, one can advise StarPU that some data will not be useful in the close future by calling <a class="el" href="group__API__Data__Management.html#gafd4b4f7f9f0a26f65a1e149525a09bfd">starpu_data_wont_use()</a>. StarPU will then write its value back to its home node, and evict it from GPUs when room is needed.</p>
<h1><a class="anchor" id="PartitioningData"></a>
Partitioning Data</h1>
<p>An existing piece of data can be partitioned in sub parts to be used by different tasks, for instance:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#define NX 1048576</span></div><div class="line"><span class="preprocessor">#define PARTS 16</span></div><div class="line"><span class="keywordtype">int</span> vector[NX];</div><div class="line"><a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> handle;</div><div class="line"></div><div class="line"><span class="comment">/* Declare data to StarPU */</span></div><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga4248716bc322e1628b86365d7b9a8822">starpu_vector_data_register</a>(&amp;handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, (uintptr_t)vector, NX, <span class="keyword">sizeof</span>(vector[0]));</div><div class="line"></div><div class="line"><span class="comment">/* Partition the vector in PARTS sub-vectors */</span></div><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Data__Partition.html#structstarpu__data__filter">starpu_data_filter</a> f =</div><div class="line">{</div><div class="line">    .<a class="code" href="group__API__Data__Partition.html#a0794c07d3fbfab35b6889760699c7b91">filter_func</a> = <a class="code" href="group__API__Data__Partition.html#ga212189d3b83dfa4e225609b5f2bf8461">starpu_vector_filter_block</a>,</div><div class="line">    .nchildren = PARTS</div><div class="line">};</div><div class="line"><a class="code" href="group__API__Data__Partition.html#ga1363109ba0e36c1b6c7f1a40c9608791">starpu_data_partition</a>(handle, &amp;f);</div></div><!-- fragment --><p>The task submission then uses the function <a class="el" href="group__API__Data__Partition.html#gac24101bbe28b1d7d4a0874d349ba8979">starpu_data_get_sub_data()</a> to retrieve the sub-handles to be passed as tasks parameters.</p>
<div class="fragment"><div class="line"><span class="comment">/* Submit a task on each sub-vector */</span></div><div class="line"><span class="keywordflow">for</span> (i=0; i&lt;<a class="code" href="group__API__Data__Partition.html#ga6a3f729055f14384e7397d2815a2c9a5">starpu_data_get_nb_children</a>(handle); i++)</div><div class="line">{</div><div class="line">    <span class="comment">/* Get subdata number i (there is only 1 dimension) */</span></div><div class="line">    <a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> sub_handle = <a class="code" href="group__API__Data__Partition.html#gac24101bbe28b1d7d4a0874d349ba8979">starpu_data_get_sub_data</a>(handle, 1, i);</div><div class="line">    <span class="keyword">struct </span><a class="code" href="group__API__Codelet__And__Tasks.html#structstarpu__task">starpu_task</a> *task = <a class="code" href="group__API__Codelet__And__Tasks.html#ga042d3b1b8083e49f2977f5032fda938c">starpu_task_create</a>();</div><div class="line"></div><div class="line">    task-&gt;<a class="code" href="group__API__Codelet__And__Tasks.html#af3ce0252f1ac2238325033386a726df3">handles</a>[0] = sub_handle;</div><div class="line">    task-&gt;<a class="code" href="group__API__Codelet__And__Tasks.html#ac0e8ab897e436b244f13ec17b1191062">cl</a> = &amp;<a class="code" href="group__API__Codelet__And__Tasks.html#ac0e8ab897e436b244f13ec17b1191062">cl</a>;</div><div class="line">    task-&gt;<a class="code" href="group__API__Codelet__And__Tasks.html#a18439d7a6d4ad65b75c75ec02d60075e">synchronous</a> = 1;</div><div class="line">    task-&gt;<a class="code" href="group__API__Codelet__And__Tasks.html#a1d6f3af0eab3a339c550a5a55fbd7ab2">cl_arg</a> = &amp;factor;</div><div class="line">    task-&gt;<a class="code" href="group__API__Codelet__And__Tasks.html#adac9af88fed23241fc65259712352126">cl_arg_size</a> = <span class="keyword">sizeof</span>(factor);</div><div class="line"></div><div class="line">    <a class="code" href="group__API__Codelet__And__Tasks.html#gaa32228bf7f452f7d664986668ea46590">starpu_task_submit</a>(task);</div><div class="line">}</div></div><!-- fragment --><p>Partitioning can be applied several times, see <code>examples/basic_examples/mult.c</code> and <code>examples/filters/</code>.</p>
<p>Wherever the whole piece of data is already available, the partitioning will be done in-place, i.e. without allocating new buffers but just using pointers inside the existing copy. This is particularly important to be aware of when using OpenCL, where the kernel parameters are not pointers, but <code>cl_mem</code> handles. The kernel thus needs to be also passed the offset within the OpenCL buffer:</p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> opencl_func(<span class="keywordtype">void</span> *buffers[], <span class="keywordtype">void</span> *cl_arg)</div><div class="line">{</div><div class="line">    cl_mem vector = (cl_mem) <a class="code" href="group__API__Data__Interfaces.html#gadccf196d01dd9c43e931827a86f886b2">STARPU_VECTOR_GET_DEV_HANDLE</a>(buffers[0]);</div><div class="line">    <span class="keywordtype">unsigned</span> offset = <a class="code" href="group__API__Data__Interfaces.html#gafd9d133c4d658b777d9800eba79765b3">STARPU_BLOCK_GET_OFFSET</a>(buffers[0]);</div><div class="line"></div><div class="line">    ...</div><div class="line">    clSetKernelArg(kernel, 0, <span class="keyword">sizeof</span>(vector), &amp;vector);</div><div class="line">    clSetKernelArg(kernel, 1, <span class="keyword">sizeof</span>(offset), &amp;offset);</div><div class="line">    ...</div><div class="line">}</div></div><!-- fragment --><p>And the kernel has to shift from the pointer passed by the OpenCL driver:</p>
<div class="fragment"><div class="line">__kernel <span class="keywordtype">void</span> opencl_kernel(__global <span class="keywordtype">int</span> *vector, <span class="keywordtype">unsigned</span> offset)</div><div class="line">{</div><div class="line">    block = (__global <span class="keywordtype">void</span> *)block + offset;</div><div class="line">    ...</div><div class="line">}</div></div><!-- fragment --><p>When the sub-data is not of the same type as the original data, the <a class="el" href="group__API__Data__Partition.html#ac53586e4f8d91898a5d2317fe269e172">starpu_data_filter::get_child_ops</a> field needs to be set appropriately for StarPU to know which type should be used.</p>
<p>StarPU provides various interfaces and filters for matrices, vectors, etc., but applications can also write their own data interfaces and filters, see <code>examples/interface</code> and <code>examples/filters/custom_mf</code> for an example, and see <a class="el" href="DataManagement.html#DefiningANewDataInterface">Defining A New Data Interface</a> and <a class="el" href="DataManagement.html#DefiningANewDataFilter">Defining A New Data Filter</a> for documentation.</p>
<h1><a class="anchor" id="AsynchronousPartitioning"></a>
Asynchronous Partitioning</h1>
<p>The partitioning functions described in the previous section are synchronous: <a class="el" href="group__API__Data__Partition.html#ga1363109ba0e36c1b6c7f1a40c9608791">starpu_data_partition()</a> and <a class="el" href="group__API__Data__Partition.html#gae80794b9cad7855a3ee54a4361f656ed">starpu_data_unpartition()</a> both wait for all the tasks currently working on the data. This can be a bottleneck for the application.</p>
<p>An asynchronous API also exists, it works only on handles with sequential consistency. The principle is to first plan the partitioning, which returns data handles of the partition, which are not functional yet. When submitting tasks, one can mix using the handles of the partition, of the whole data. One can even partition recursively and mix using handles at different levels of the recursion. Of course, StarPU will have to introduce coherency synchronization.</p>
<p><code>fmultiple_submit_implicit</code> is a complete example using this technique. One can also look at <code>fmultiple_submit_readonly</code> which contains the explicit coherency synchronization which are automatically introduced by StarPU for <code>fmultiple_submit_implicit</code>.</p>
<p>In short, we first register a matrix and plan the partitioning:</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga833b734aa76cdee89245cb0710793cf9">starpu_matrix_data_register</a>(&amp;handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, (uintptr_t)matrix, NX, NX, NY, <span class="keyword">sizeof</span>(matrix[0]));</div><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Data__Partition.html#structstarpu__data__filter">starpu_data_filter</a> f_vert =</div><div class="line">{</div><div class="line">        .<a class="code" href="group__API__Data__Partition.html#a0794c07d3fbfab35b6889760699c7b91">filter_func</a> = <a class="code" href="group__API__Data__Partition.html#ga8c86b2af9e0806e631c1cbb5d506506b">starpu_matrix_filter_block</a>,</div><div class="line">        .nchildren = PARTS</div><div class="line">};</div><div class="line"><a class="code" href="group__API__Data__Partition.html#gaa4407a8734e1fbdbb63b83351769476c">starpu_data_partition_plan</a>(handle, &amp;f_vert, vert_handle);</div></div><!-- fragment --><p><a class="el" href="group__API__Data__Partition.html#gaa4407a8734e1fbdbb63b83351769476c">starpu_data_partition_plan()</a> returns the handles for the partition in <code>vert_handle</code>.</p>
<p>One can then submit tasks working on the main handle, and tasks working on <code>vert_handle</code> handles. Between using the main handle and <code>vert_handle</code> handles, StarPU will automatically call <a class="el" href="group__API__Data__Partition.html#ga994cbae9c619b070f8d219f6bfffff06">starpu_data_partition_submit()</a> and <a class="el" href="group__API__Data__Partition.html#ga46d2b144a7de2e17d17b1383ef5f522d">starpu_data_unpartition_submit()</a>.</p>
<p>All this code is asynchronous, just submitting which tasks, partitioning and unpartitioning will be done at runtime.</p>
<p>Planning several partitioning of the same data is also possible, StarPU will unpartition and repartition as needed when mixing accesses of different partitions. If data access is done in read-only mode, StarPU will allow the different partitioning to coexist. As soon as a data is accessed in read-write mode, StarPU will automatically unpartition everything and activate only the partitioning leading to the data being written to.</p>
<p>For instance, for a stencil application, one can split a subdomain into its interior and halos, and then just submit a task updating the whole subdomain, then submit MPI sends/receives to update the halos, then submit again a task updating the whole subdomain, etc. and StarPU will automatically partition/unpartition each time.</p>
<h1><a class="anchor" id="ManualPartitioning"></a>
Manual Partitioning</h1>
<p>One can also handle partitioning by hand, by registering several views on the same piece of data. The idea is then to manage the coherency of the various views through the common buffer in the main memory. <code>fmultiple_manual</code> is a complete example using this technique.</p>
<p>In short, we first register the same matrix several times:</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga833b734aa76cdee89245cb0710793cf9">starpu_matrix_data_register</a>(&amp;handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, (uintptr_t)matrix, NX, NX, NY, <span class="keyword">sizeof</span>(matrix[0]));</div><div class="line"></div><div class="line"><span class="keywordflow">for</span> (i = 0; i &lt; PARTS; i++)</div><div class="line">        <a class="code" href="group__API__Data__Interfaces.html#ga833b734aa76cdee89245cb0710793cf9">starpu_matrix_data_register</a>(&amp;vert_handle[i], <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, (uintptr_t)&amp;matrix[0][i*(NX/PARTS)], NX, NX/PARTS, NY, <span class="keyword">sizeof</span>(matrix[0][0]));</div></div><!-- fragment --><p>Since StarPU is not aware that the two handles are actually pointing to the same data, we have a danger of inadvertently submitting tasks to both views, which will bring a mess since StarPU will not guarantee any coherency between the two views. To make sure we don't do this, we invalidate the view that we will not use:</p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span> (i = 0; i &lt; PARTS; i++)</div><div class="line">        <a class="code" href="group__API__Data__Management.html#ga4e82fe020ec010bcacb6aee16021607c">starpu_data_invalidate</a>(vert_handle[i]);</div></div><!-- fragment --><p>Then we can safely work on <code>handle</code>.</p>
<p>When we want to switch to the vertical slice view, all we need to do is bring coherency between them by running an empty task on the home node of the data:</p>
<div class="fragment"><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Codelet__And__Tasks.html#structstarpu__codelet">starpu_codelet</a> cl_switch =</div><div class="line">{</div><div class="line">        .<a class="code" href="group__API__Codelet__And__Tasks.html#a504b8e20f53f469358eadb1ed800f72c">where</a> = <a class="code" href="group__API__Codelet__And__Tasks.html#ga7d5977bc7532b4b357ea8b1017b5aaca">STARPU_NOWHERE</a>,</div><div class="line">        .nbuffers = 3,</div><div class="line">        .specific_nodes = 1,</div><div class="line">        .nodes = { <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, STARPU_MAIN_RAM },</div><div class="line">};</div><div class="line"></div><div class="line">ret = <a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;cl_switch, STARPU_RW, handle,</div><div class="line">                        <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba628e5483f4cb6e44779aea39300dafed">STARPU_W</a>, vert_handle[0],</div><div class="line">                        <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba628e5483f4cb6e44779aea39300dafed">STARPU_W</a>, vert_handle[1],</div><div class="line">                        0);</div></div><!-- fragment --><p>The execution of the <code>switch</code> task will get back the matrix data into the main memory, and thus the vertical slices will get the updated value there.</p>
<p>Again, we prefer to make sure that we don't accidentally access the matrix through the whole-matrix handle:</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Management.html#ga06b01fdf769f8f2eb222ecde42afbc81">starpu_data_invalidate_submit</a>(handle);</div></div><!-- fragment --><p>Note: when enabling a set of handles in this way, the set must not have any overlapping, i.e. the handles of the set must not have any part of data in common, otherwise StarPU will not properly handle concurrent accesses between them.</p>
<p>And now we can start using vertical slices, etc.</p>
<h1><a class="anchor" id="DataPointers"></a>
Handles data buffer pointers</h1>
<p>A simple understanding of starpu handles is that it's a collection of buffers on each memory node of the machine, which contain the same data. The picture is however made more complex with the OpenCL support and with partitioning.</p>
<p>When partitioning a handle, the data buffers of the subhandles will indeed be inside the data buffers of the main handle (to save transferring data back and forth between the main handle and the subhandles). But in OpenCL, a <code>cl_mem</code> is not a pointer, but an opaque value on which pointer arithmetic can not be used. That is why data interfaces contain three members: <code>dev_handle</code>, <code>offset</code>, and <code>ptr</code>. The <code>dev_handle</code> member is what the allocation function returned, and one can not do arithmetic on it. The <code>offset</code> member is the offset inside the allocated area, most often it will be 0 because data start at the beginning of the allocated area, but when the handle is partitioned, the subhandles will have varying <code>offset</code> values, for each subpiece. The <code>ptr</code> member, in the non-OpenCL case, i.e. when pointer arithmetic can be used on <code>dev_handle</code>, is just the sum of <code>dev_handle</code> and <code>offset</code>, provided for convenience.</p>
<p>This means that: </p><ul>
<li>
computation kernels can use <code>ptr</code> in non-OpenCL implementations. </li>
<li>
computation kernels have to use <code>dev_handle</code> and <code>offset</code> in the OpenCL implementation. </li>
<li>
allocation methods of data interfaces have to store the value returned by starpu_malloc_on_node in <code>dev_handle</code> and <code>ptr</code>, and set <code>offset</code> to 0. </li>
<li>
partitioning filters have to copy over <code>dev_handle</code> without modifying it, set in the child different values of <code>offset</code>, and set <code>ptr</code> accordingly as the sum of <code>dev_handle</code> and <code>offset</code>. </li>
</ul>
<h1><a class="anchor" id="DefiningANewDataFilter"></a>
Defining A New Data Filter</h1>
<p>StarPU provides a series of predefined filters in <a class="el" href="group__API__Data__Partition.html">Data Partition</a>, but additional filters can be defined by the application. The principle is that the filter function just fills the memory location of the <code>i-th</code> subpart of a data. Examples are provided in <code>src/datawizard/interfaces/*_filters.c</code>, and see <a class="el" href="group__API__Data__Partition.html#a0794c07d3fbfab35b6889760699c7b91">starpu_data_filter::filter_func</a> for the details. The <a class="el" href="group__API__Data__Partition.html#gacd670ea4e020b6ec14b3fe8bf8361582">starpu_filter_nparts_compute_chunk_size_and_offset()</a> helper can be used to compute the division of pieces of data.</p>
<h1><a class="anchor" id="DataReduction"></a>
Data Reduction</h1>
<p>In various cases, some piece of data is used to accumulate intermediate results. For instances, the dot product of a vector, maximum/minimum finding, the histogram of a photograph, etc. When these results are produced along the whole machine, it would not be efficient to accumulate them in only one place, incurring data transmission each and access concurrency.</p>
<p>StarPU provides a mode <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaa92c1e935fb451cc339a21976680ee05">STARPU_REDUX</a>, which permits to optimize this case: it will allocate a buffer on each worker (lazily), and accumulate intermediate results there. When the data is eventually accessed in the normal mode <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, StarPU will collect the intermediate results in just one buffer.</p>
<p>For this to work, the user has to use the function <a class="el" href="group__API__Data__Management.html#ga1f5ba1c1cfefc1f81a4095cf3c213e54">starpu_data_set_reduction_methods()</a> to declare how to initialize these buffers, and how to assemble partial results.</p>
<p>For instance, <code>cg</code> uses that to optimize its dot product: it first defines the codelets for initialization and reduction:</p>
<div class="fragment"><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Codelet__And__Tasks.html#structstarpu__codelet">starpu_codelet</a> bzero_variable_cl =</div><div class="line">{</div><div class="line">        .<a class="code" href="group__API__Codelet__And__Tasks.html#a593418a8961318e4085177abeeaa43ad">cpu_funcs</a> = { bzero_variable_cpu },</div><div class="line">        .cpu_funcs_name = { <span class="stringliteral">&quot;bzero_variable_cpu&quot;</span> },</div><div class="line">        .cuda_funcs = { bzero_variable_cuda },</div><div class="line">        .nbuffers = 1,</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">static</span> <span class="keywordtype">void</span> accumulate_variable_cpu(<span class="keywordtype">void</span> *descr[], <span class="keywordtype">void</span> *cl_arg)</div><div class="line">{</div><div class="line">        <span class="keywordtype">double</span> *v_dst = (<span class="keywordtype">double</span> *)<a class="code" href="group__API__Data__Interfaces.html#gad99e209fafc3ad5e0c4da0de02e5bda2">STARPU_VARIABLE_GET_PTR</a>(descr[0]);</div><div class="line">        <span class="keywordtype">double</span> *v_src = (<span class="keywordtype">double</span> *)<a class="code" href="group__API__Data__Interfaces.html#gad99e209fafc3ad5e0c4da0de02e5bda2">STARPU_VARIABLE_GET_PTR</a>(descr[1]);</div><div class="line">        *v_dst = *v_dst + *v_src;</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">static</span> <span class="keywordtype">void</span> accumulate_variable_cuda(<span class="keywordtype">void</span> *descr[], <span class="keywordtype">void</span> *cl_arg)</div><div class="line">{</div><div class="line">        <span class="keywordtype">double</span> *v_dst = (<span class="keywordtype">double</span> *)<a class="code" href="group__API__Data__Interfaces.html#gad99e209fafc3ad5e0c4da0de02e5bda2">STARPU_VARIABLE_GET_PTR</a>(descr[0]);</div><div class="line">        <span class="keywordtype">double</span> *v_src = (<span class="keywordtype">double</span> *)<a class="code" href="group__API__Data__Interfaces.html#gad99e209fafc3ad5e0c4da0de02e5bda2">STARPU_VARIABLE_GET_PTR</a>(descr[1]);</div><div class="line">        cublasaxpy(1, (<span class="keywordtype">double</span>)1.0, v_src, 1, v_dst, 1);</div><div class="line">        cudaStreamSynchronize(<a class="code" href="group__API__CUDA__Extensions.html#gad7d80d054bd2b9570e1d7e24442e19c0">starpu_cuda_get_local_stream</a>());</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Codelet__And__Tasks.html#structstarpu__codelet">starpu_codelet</a> accumulate_variable_cl =</div><div class="line">{</div><div class="line">        .<a class="code" href="group__API__Codelet__And__Tasks.html#a593418a8961318e4085177abeeaa43ad">cpu_funcs</a> = { accumulate_variable_cpu },</div><div class="line">        .cpu_funcs_name = { <span class="stringliteral">&quot;accumulate_variable_cpu&quot;</span> },</div><div class="line">        .cuda_funcs = { accumulate_variable_cuda },</div><div class="line">        .nbuffers = 1,</div><div class="line">}</div></div><!-- fragment --><p>and attaches them as reduction methods for its handle <code>dtq</code>:</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Interfaces.html#gafb92c2538dc629d823221c5dc16bd767">starpu_variable_data_register</a>(&amp;dtq_handle, -1, NULL, <span class="keyword">sizeof</span>(<a class="code" href="group__API__Codelet__And__Tasks.html#af46aad8aa73b1a3fa855606d2d1a7cdd">type</a>));</div><div class="line"><a class="code" href="group__API__Data__Management.html#ga1f5ba1c1cfefc1f81a4095cf3c213e54">starpu_data_set_reduction_methods</a>(dtq_handle, &amp;accumulate_variable_cl, &amp;bzero_variable_cl);</div></div><!-- fragment --><p>and <code>dtq_handle</code> can now be used in mode <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaa92c1e935fb451cc339a21976680ee05">STARPU_REDUX</a> for the dot products with partitioned vectors:</p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span> (b = 0; b &lt; nblocks; b++)</div><div class="line">    <a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;dot_kernel_cl,</div><div class="line">        <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaa92c1e935fb451cc339a21976680ee05">STARPU_REDUX</a>, dtq_handle,</div><div class="line">        <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, <a class="code" href="group__API__Data__Partition.html#gac24101bbe28b1d7d4a0874d349ba8979">starpu_data_get_sub_data</a>(v1, 1, b),</div><div class="line">        <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, <a class="code" href="group__API__Data__Partition.html#gac24101bbe28b1d7d4a0874d349ba8979">starpu_data_get_sub_data</a>(v2, 1, b),</div><div class="line">        0);</div></div><!-- fragment --><p>During registration, we have here provided <code>NULL</code>, i.e. there is no initial value to be taken into account during reduction. StarPU will thus only take into account the contributions from the tasks <code>dot_kernel_cl</code>. Also, it will not allocate any memory for <code>dtq_handle</code> before tasks <code>dot_kernel_cl</code> are ready to run.</p>
<p>If another dot product has to be performed, one could unregister <code>dtq_handle</code>, and re-register it. But one can also call <a class="el" href="group__API__Data__Management.html#ga06b01fdf769f8f2eb222ecde42afbc81">starpu_data_invalidate_submit()</a> with the parameter <code>dtq_handle</code>, which will clear all data from the handle, thus resetting it back to the initial status <code>register(NULL)</code>.</p>
<p>The example <code>cg</code> also uses reduction for the blocked gemv kernel, leading to yet more relaxed dependencies and more parallelism.</p>
<p><a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaa92c1e935fb451cc339a21976680ee05">STARPU_REDUX</a> can also be passed to <a class="el" href="group__API__MPI__Support.html#gaa823d6398e61516bba887b90ad048914">starpu_mpi_task_insert()</a> in the MPI case. This will however not produce any MPI communication, but just pass <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaa92c1e935fb451cc339a21976680ee05">STARPU_REDUX</a> to the underlying <a class="el" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert()</a>. It is up to the application to call <a class="el" href="group__API__MPI__Support.html#ga565996704c6707a55410488f8d569357">starpu_mpi_redux_data()</a>, which posts tasks which will reduce the partial results among MPI nodes into the MPI node which owns the data. For instance, some hypothetical application which collects partial results into data <code>res</code>, then uses it for other computation, before looping again with a new reduction:</p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span> (i = 0; i &lt; 100; i++)</div><div class="line">{</div><div class="line">    <a class="code" href="group__API__MPI__Support.html#gaa823d6398e61516bba887b90ad048914">starpu_mpi_task_insert</a>(MPI_COMM_WORLD, &amp;init_res, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba628e5483f4cb6e44779aea39300dafed">STARPU_W</a>, res, 0);</div><div class="line">    <a class="code" href="group__API__MPI__Support.html#gaa823d6398e61516bba887b90ad048914">starpu_mpi_task_insert</a>(MPI_COMM_WORLD, &amp;work, STARPU_RW, A, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, B, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaa92c1e935fb451cc339a21976680ee05">STARPU_REDUX</a>, res, 0);</div><div class="line">    <a class="code" href="group__API__MPI__Support.html#ga565996704c6707a55410488f8d569357">starpu_mpi_redux_data</a>(MPI_COMM_WORLD, res);</div><div class="line">    <a class="code" href="group__API__MPI__Support.html#gaa823d6398e61516bba887b90ad048914">starpu_mpi_task_insert</a>(MPI_COMM_WORLD, &amp;work2, STARPU_RW, B, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, res, 0);</div><div class="line">}</div></div><!-- fragment --><h1><a class="anchor" id="DataCommute"></a>
Commute Data Access</h1>
<p>By default, the implicit dependencies computed from data access use the sequential semantic. Notably, write accesses are always serialized in the order of submission. In some applicative cases, the write contributions can actually be performed in any order without affecting the eventual result. In this case it is useful to drop the strictly sequential semantic, to improve parallelism by allowing StarPU to reorder the write accesses. This can be done by using the <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbade64878c777f3655d8802cbc6b48a95d">STARPU_COMMUTE</a> data access flag. Accesses without this flag will however properly be serialized against accesses with this flag. For instance:</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;cl1, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, h, STARPU_RW, handle, 0);</div><div class="line"><a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;cl2, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, handle1, STARPU_RW|<a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbade64878c777f3655d8802cbc6b48a95d">STARPU_COMMUTE</a>, handle, 0);</div><div class="line"><a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;cl2, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, handle2, STARPU_RW|<a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbade64878c777f3655d8802cbc6b48a95d">STARPU_COMMUTE</a>, handle, 0);</div><div class="line"><a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;cl3, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, g, STARPU_RW, handle, 0);</div></div><!-- fragment --><p>The two tasks running <code>cl2</code> will be able to commute: depending on whether the value of <code>handle1</code> or <code>handle2</code> becomes available first, the corresponding task running <code>cl2</code> will start first. The task running <code>cl1</code> will however always be run before them, and the task running <code>cl3</code> will always be run after them.</p>
<p>If a lot of tasks use the commute access on the same set of data and a lot of them are ready at the same time, it may become interesting to use an arbiter, see <a class="el" href="DataManagement.html#ConcurrentDataAccess">Concurrent Data Accesses</a>.</p>
<h1><a class="anchor" id="ConcurrentDataAccess"></a>
Concurrent Data Accesses</h1>
<p>When several tasks are ready and will work on several data, StarPU is faced with the classical Dining Philosophers problem, and has to determine the order in which it will run the tasks.</p>
<p>Data accesses usually use sequential ordering, so data accesses are usually already serialized, and thus by default StarPU uses the Dijkstra solution which scales very well in terms of overhead: tasks will just acquire data one by one by data handle pointer value order.</p>
<p>When sequential ordering is disabled or the <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbade64878c777f3655d8802cbc6b48a95d">STARPU_COMMUTE</a> flag is used, there may be a lot of concurrent accesses to the same data, and the Dijkstra solution gets only poor parallelism, typically in some pathological cases which do happen in various applications. In this case, one can use a data access arbiter, which implements the classical centralized solution for the Dining Philosophers problem. This is more expensive in terms of overhead since it is centralized, but it opportunistically gets a lot of parallelism. The centralization can also be avoided by using several arbiters, thus separating sets of data for which arbitration will be done. If a task accesses data from different arbiters, it will acquire them arbiter by arbiter, in arbiter pointer value order.</p>
<p>See the <code>tests/datawizard/test_arbiter.cpp</code> example.</p>
<p>Arbiters however do not support the <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaa92c1e935fb451cc339a21976680ee05">STARPU_REDUX</a> flag yet.</p>
<h1><a class="anchor" id="TemporaryBuffers"></a>
Temporary Buffers</h1>
<p>There are two kinds of temporary buffers: temporary data which just pass results from a task to another, and scratch data which are needed only internally by tasks.</p>
<h2><a class="anchor" id="TemporaryData"></a>
Temporary Data</h2>
<p>Data can sometimes be entirely produced by a task, and entirely consumed by another task, without the need for other parts of the application to access it. In such case, registration can be done without prior allocation, by using the special memory node number <code>-1</code>, and passing a zero pointer. StarPU will actually allocate memory only when the task creating the content gets scheduled, and destroy it on unregistration.</p>
<p>In addition to this, it can be tedious for the application to have to unregister the data, since it will not use its content anyway. The unregistration can be done lazily by using the function <a class="el" href="group__API__Data__Management.html#ga4fa34753bff1d29c20f0a0e361020b4e">starpu_data_unregister_submit()</a>, which will record that no more tasks accessing the handle will be submitted, so that it can be freed as soon as the last task accessing it is over.</p>
<p>The following code examplifies both points: it registers the temporary data, submits three tasks accessing it, and records the data for automatic unregistration.</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga4248716bc322e1628b86365d7b9a8822">starpu_vector_data_register</a>(&amp;handle, -1, 0, n, <span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div><div class="line"><a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;produce_data, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba628e5483f4cb6e44779aea39300dafed">STARPU_W</a>, handle, 0);</div><div class="line"><a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;compute_data, STARPU_RW, handle, 0);</div><div class="line"><a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;summarize_data, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, handle, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba628e5483f4cb6e44779aea39300dafed">STARPU_W</a>, result_handle, 0);</div><div class="line"><a class="code" href="group__API__Data__Management.html#ga4fa34753bff1d29c20f0a0e361020b4e">starpu_data_unregister_submit</a>(handle);</div></div><!-- fragment --><p>The application may also want to see the temporary data initialized on the fly before being used by the task. This can be done by using <a class="el" href="group__API__Data__Management.html#ga1f5ba1c1cfefc1f81a4095cf3c213e54">starpu_data_set_reduction_methods()</a> to set an initialization codelet (no redux codelet is needed).</p>
<h2><a class="anchor" id="ScratchData"></a>
Scratch Data</h2>
<p>Some kernels sometimes need temporary data to achieve the computations, i.e. a workspace. The application could allocate it at the start of the codelet function, and free it at the end, but this would be costly. It could also allocate one buffer per worker (similarly to <a class="el" href="FrequentlyAskedQuestions.html#HowToInitializeAComputationLibraryOnceForEachWorker">How To Initialize A Computation Library Once For Each Worker?</a>), but this would make them systematic and permanent. A more optimized way is to use the data access mode <a class="el" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba5531afb0a53e301d4bff753fed705179">STARPU_SCRATCH</a>, as examplified below, which provides per-worker buffers without content consistency. The buffer is registered only once, using memory node <code>-1</code>, i.e. the application didn't allocate memory for it, and StarPU will allocate it on demand at task execution.</p>
<div class="fragment"><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga4248716bc322e1628b86365d7b9a8822">starpu_vector_data_register</a>(&amp;workspace, -1, 0, <span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div><div class="line"><span class="keywordflow">for</span> (i = 0; i &lt; N; i++)</div><div class="line">    <a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;compute, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, input[i], <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba5531afb0a53e301d4bff753fed705179">STARPU_SCRATCH</a>, workspace, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba628e5483f4cb6e44779aea39300dafed">STARPU_W</a>, output[i], 0);</div></div><!-- fragment --><p>StarPU will make sure that the buffer is allocated before executing the task, and make this allocation per-worker: for CPU workers, notably, each worker has its own buffer. This means that each task submitted above will actually have its own workspace, which will actually be the same for all tasks running one after the other on the same worker. Also, if for instance memory becomes scarce, StarPU will notice that it can free such buffers easily, since the content does not matter.</p>
<p>The example <code>examples/pi</code> uses scratches for some temporary buffer.</p>
<h1><a class="anchor" id="TheMultiformatInterface"></a>
The Multiformat Interface</h1>
<p>It may be interesting to represent the same piece of data using two different data structures: one only used on CPUs, and one only used on GPUs. This can be done by using the multiformat interface. StarPU will be able to convert data from one data structure to the other when needed. Note that the scheduler <code>dmda</code> is the only one optimized for this interface. The user must provide StarPU with conversion codelets:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#define NX 1024</span></div><div class="line"><span class="keyword">struct </span>point array_of_structs[NX];</div><div class="line"><a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> handle;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment"> * The conversion of a piece of data is itself a task, though it is created,</span></div><div class="line"><span class="comment"> * submitted and destroyed by StarPU internals and not by the user. Therefore,</span></div><div class="line"><span class="comment"> * we have to define two codelets.</span></div><div class="line"><span class="comment"> * Note that for now the conversion from the CPU format to the GPU format has to</span></div><div class="line"><span class="comment"> * be executed on the GPU, and the conversion from the GPU to the CPU has to be</span></div><div class="line"><span class="comment"> * executed on the CPU.</span></div><div class="line"><span class="comment"> */</span></div><div class="line"><span class="preprocessor">#ifdef STARPU_USE_OPENCL</span></div><div class="line"><span class="keywordtype">void</span> cpu_to_opencl_opencl_func(<span class="keywordtype">void</span> *buffers[], <span class="keywordtype">void</span> *args);</div><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Codelet__And__Tasks.html#structstarpu__codelet">starpu_codelet</a> cpu_to_opencl_cl =</div><div class="line">{</div><div class="line">    .<a class="code" href="group__API__Codelet__And__Tasks.html#a504b8e20f53f469358eadb1ed800f72c">where</a> = <a class="code" href="group__API__Codelet__And__Tasks.html#gaec9e5cdf8ac48607ce4495ded31001d5">STARPU_OPENCL</a>,</div><div class="line">    .opencl_funcs = { cpu_to_opencl_opencl_func },</div><div class="line">    .nbuffers = 1,</div><div class="line">    .modes = { STARPU_RW }</div><div class="line">};</div><div class="line"></div><div class="line"><span class="keywordtype">void</span> opencl_to_cpu_func(<span class="keywordtype">void</span> *buffers[], <span class="keywordtype">void</span> *args);</div><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Codelet__And__Tasks.html#structstarpu__codelet">starpu_codelet</a> opencl_to_cpu_cl =</div><div class="line">{</div><div class="line">    .<a class="code" href="group__API__Codelet__And__Tasks.html#a504b8e20f53f469358eadb1ed800f72c">where</a> = <a class="code" href="group__API__Codelet__And__Tasks.html#gaf577e4415a639beffbc48b65454b88ca">STARPU_CPU</a>,</div><div class="line">    .cpu_funcs = { opencl_to_cpu_func },</div><div class="line">    .cpu_funcs_name = { <span class="stringliteral">&quot;opencl_to_cpu_func&quot;</span> },</div><div class="line">    .nbuffers = 1,</div><div class="line">    .modes = { STARPU_RW }</div><div class="line">};</div><div class="line"><span class="preprocessor">#endif</span></div><div class="line"></div><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Data__Interfaces.html#structstarpu__multiformat__data__interface__ops">starpu_multiformat_data_interface_ops</a> format_ops =</div><div class="line">{</div><div class="line"><span class="preprocessor">#ifdef STARPU_USE_OPENCL</span></div><div class="line">    .<a class="code" href="group__API__Data__Interfaces.html#a7c0e09ea6559bef4579a1fb1e57f67d2">opencl_elemsize</a> = 2 * <span class="keyword">sizeof</span>(float),</div><div class="line">    .cpu_to_opencl_cl = &amp;cpu_to_opencl_cl,</div><div class="line">    .opencl_to_cpu_cl = &amp;opencl_to_cpu_cl,</div><div class="line">#endif</div><div class="line">    .<a class="code" href="group__API__Data__Interfaces.html#a67a1f3202dfc2512ed7ed14bed4f3623">cpu_elemsize</a> = 2 * <span class="keyword">sizeof</span>(<span class="keywordtype">float</span>),</div><div class="line">    ...</div><div class="line">};</div><div class="line"></div><div class="line"><a class="code" href="group__API__Data__Interfaces.html#ga359c851b4a4b0adb9685ade2f78818d9">starpu_multiformat_data_register</a>(handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, &amp;array_of_structs, NX, &amp;format_ops);</div></div><!-- fragment --><p> Kernels can be written almost as for any other interface. Note that <a class="el" href="group__API__Data__Interfaces.html#gaf0937aae4bedeb03e9ebbec36cc1dad6">STARPU_MULTIFORMAT_GET_CPU_PTR</a> shall only be used for CPU kernels. CUDA kernels must use <a class="el" href="group__API__Data__Interfaces.html#gaeb6356075b3f57e8c7aeccc85f076d87">STARPU_MULTIFORMAT_GET_CUDA_PTR</a>, and OpenCL kernels must use <a class="el" href="group__API__Data__Interfaces.html#ga0b6869745c464525b25a0757d112c318">STARPU_MULTIFORMAT_GET_OPENCL_PTR</a>. <a class="el" href="group__API__Data__Interfaces.html#ga9719eab3a5e9fbaad2837b4a985a3a14">STARPU_MULTIFORMAT_GET_NX</a> may be used in any kind of kernel.</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">void</span></div><div class="line">multiformat_scal_cpu_func(<span class="keywordtype">void</span> *buffers[], <span class="keywordtype">void</span> *args)</div><div class="line">{</div><div class="line">    <span class="keyword">struct </span>point *aos;</div><div class="line">    <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n;</div><div class="line"></div><div class="line">    aos = <a class="code" href="group__API__Data__Interfaces.html#gaf0937aae4bedeb03e9ebbec36cc1dad6">STARPU_MULTIFORMAT_GET_CPU_PTR</a>(buffers[0]);</div><div class="line">    n = <a class="code" href="group__API__Data__Interfaces.html#ga9719eab3a5e9fbaad2837b4a985a3a14">STARPU_MULTIFORMAT_GET_NX</a>(buffers[0]);</div><div class="line">    ...</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">extern</span> <span class="stringliteral">&quot;C&quot;</span> <span class="keywordtype">void</span> multiformat_scal_cuda_func(<span class="keywordtype">void</span> *buffers[], <span class="keywordtype">void</span> *_args)</div><div class="line">{</div><div class="line">    <span class="keywordtype">unsigned</span> <span class="keywordtype">int</span> n;</div><div class="line">    <span class="keyword">struct </span>struct_of_arrays *soa;</div><div class="line"></div><div class="line">    soa = (<span class="keyword">struct </span>struct_of_arrays *) <a class="code" href="group__API__Data__Interfaces.html#gaeb6356075b3f57e8c7aeccc85f076d87">STARPU_MULTIFORMAT_GET_CUDA_PTR</a>(buffers[0]);</div><div class="line">    n = <a class="code" href="group__API__Data__Interfaces.html#ga9719eab3a5e9fbaad2837b4a985a3a14">STARPU_MULTIFORMAT_GET_NX</a>(buffers[0]);</div><div class="line"></div><div class="line">    ...</div><div class="line">}</div></div><!-- fragment --><p>A full example may be found in <code>examples/basic_examples/multiformat.c</code>.</p>
<h1><a class="anchor" id="DefiningANewDataInterface"></a>
Defining A New Data Interface</h1>
<p>This section proposes an example how to define your own interface, when the StarPU-provided interface do not fit your needs. Here we take a dumb example of an array of complex numbers represented by two arrays of double values.</p>
<p>Let's thus define a new data interface to manage arrays of complex numbers:</p>
<div class="fragment"><div class="line"><span class="comment">/* interface for complex numbers */</span></div><div class="line"><span class="keyword">struct </span>starpu_complex_interface</div><div class="line">{</div><div class="line">        <span class="keywordtype">double</span> *real;</div><div class="line">        <span class="keywordtype">double</span> *imaginary;</div><div class="line">        <span class="keywordtype">int</span> nx;</div><div class="line">};</div></div><!-- fragment --><p>That structure stores enough to describe <b>one</b> buffer of such kind of data. It is used for the buffer stored in the main memory, another instance is used for the buffer stored in a GPU, etc. A <em>data handle</em> is thus a collection of such structures, to remember each buffer on each memory node.</p>
<p>Note: one should not take pointers into such structures, because StarPU needs to be able to copy over the content of it to various places, for instance to efficiently migrate a data buffer from one data handle to another data handle.</p>
<h2><a class="anchor" id="DefiningANewDataInterface_registration"></a>
Data registration</h2>
<p>Registering such a data to StarPU is easily done using the function <a class="el" href="group__API__Data__Interfaces.html#ga7ea069447f11e836d7b709c97d90b570">starpu_data_register()</a>. The last parameter of the function, <code>interface_complex_ops</code>, will be described below.</p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> starpu_complex_data_register(<a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> *handle,</div><div class="line">     <span class="keywordtype">unsigned</span> home_node, <span class="keywordtype">double</span> *real, <span class="keywordtype">double</span> *imaginary, <span class="keywordtype">int</span> nx)</div><div class="line">{</div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface complex =</div><div class="line">        {</div><div class="line">                .real = real,</div><div class="line">                .imaginary = imaginary,</div><div class="line">                .nx = nx</div><div class="line">        };</div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (interface_complex_ops.interfaceid == <a class="code" href="group__API__Data__Interfaces.html#ggaa2f2140147f15e7b9eec1443690e357ca65cfc4ac63a449dd5da3c40ae63bb949">STARPU_UNKNOWN_INTERFACE_ID</a>)</div><div class="line">        {</div><div class="line">                interface_complex_ops.interfaceid = <a class="code" href="group__API__Data__Interfaces.html#gaf5ea640f2c977e3ae95a6be9b3be3bee">starpu_data_interface_get_next_id</a>();</div><div class="line">        }</div><div class="line"></div><div class="line">        <a class="code" href="group__API__Data__Interfaces.html#ga7ea069447f11e836d7b709c97d90b570">starpu_data_register</a>(handleptr, home_node, &amp;complex, &amp;interface_complex_ops);</div><div class="line">}</div></div><!-- fragment --><p>The <code>struct starpu_complex_interface complex</code> is here used just to store the parameters that the user provided to <code>starpu_complex_data_register</code>. <a class="el" href="group__API__Data__Interfaces.html#ga7ea069447f11e836d7b709c97d90b570">starpu_data_register()</a> will first allocate the handle, and then pass the <code>starpu_complex_interface</code> structure to the <a class="el" href="group__API__Data__Interfaces.html#ab92c5c96a67820888d933af69029d8b1">starpu_data_interface_ops::register_data_handle</a> method, which records them within the data handle (it is called once per node by <a class="el" href="group__API__Data__Interfaces.html#ga7ea069447f11e836d7b709c97d90b570">starpu_data_register()</a>):</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">void</span> complex_register_data_handle(<a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> handle, <span class="keywordtype">unsigned</span> home_node, <span class="keywordtype">void</span> *data_interface)</div><div class="line">{</div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface *complex_interface = (<span class="keyword">struct </span>starpu_complex_interface *) data_interface;</div><div class="line"></div><div class="line">        <span class="keywordtype">unsigned</span> node;</div><div class="line">        <span class="keywordflow">for</span> (node = 0; node &lt; <a class="code" href="group__API__Workers__Properties.html#gaa7a73dac05f65d5760abf29016e251f0">STARPU_MAXNODES</a>; node++)</div><div class="line">        {</div><div class="line">                <span class="keyword">struct </span>starpu_complex_interface *local_interface = (<span class="keyword">struct </span>starpu_complex_interface *)</div><div class="line">                        <a class="code" href="group__API__Data__Interfaces.html#ga357281162710186412327c17f4a63535">starpu_data_get_interface_on_node</a>(handle, node);</div><div class="line"></div><div class="line">                local_interface-&gt;nx = complex_interface-&gt;nx;</div><div class="line">                <span class="keywordflow">if</span> (node == home_node)</div><div class="line">                {</div><div class="line">                        local_interface-&gt;real = complex_interface-&gt;real;</div><div class="line">                        local_interface-&gt;imaginary = complex_interface-&gt;imaginary;</div><div class="line">                }</div><div class="line">                <span class="keywordflow">else</span></div><div class="line">                {</div><div class="line">                        local_interface-&gt;real = NULL;</div><div class="line">                        local_interface-&gt;imaginary = NULL;</div><div class="line">                }</div><div class="line">        }</div><div class="line">}</div></div><!-- fragment --><p>If the application provided a home node, the corresponding pointers will be recorded for that node. Others have no buffer allocated yet. Possibly the interface needs some dynamic allocation (e.g. to store an array of dimensions that can have variable size). The corresponding deallocation will then be done in <a class="el" href="group__API__Data__Interfaces.html#a08ea385f365a2c0e36bcbadaf229d2fa">starpu_data_interface_ops::unregister_data_handle</a>.</p>
<p>Different operations need to be defined for a data interface through the type <a class="el" href="group__API__Data__Interfaces.html#structstarpu__data__interface__ops">starpu_data_interface_ops</a>. We only define here the basic operations needed to run simple applications. The source code for the different functions can be found in the file <code>examples/interface/complex_interface.c</code>, the details of the hooks to be provided are documented in <a class="el" href="group__API__Data__Interfaces.html#structstarpu__data__interface__ops">starpu_data_interface_ops</a> .</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keyword">struct </span><a class="code" href="group__API__Data__Interfaces.html#structstarpu__data__interface__ops">starpu_data_interface_ops</a> interface_complex_ops =</div><div class="line">{</div><div class="line">        .<a class="code" href="group__API__Data__Interfaces.html#ab92c5c96a67820888d933af69029d8b1">register_data_handle</a> = complex_register_data_handle,</div><div class="line">        .allocate_data_on_node = complex_allocate_data_on_node,</div><div class="line">        .copy_methods = &amp;complex_copy_methods,</div><div class="line">        .get_size = complex_get_size,</div><div class="line">        .footprint = complex_footprint,</div><div class="line">        .interfaceid = <a class="code" href="group__API__Data__Interfaces.html#ggaa2f2140147f15e7b9eec1443690e357ca65cfc4ac63a449dd5da3c40ae63bb949">STARPU_UNKNOWN_INTERFACE_ID</a>,</div><div class="line">        .interface_size = <span class="keyword">sizeof</span>(<span class="keyword">struct </span>starpu_complex_interface),</div><div class="line">};</div></div><!-- fragment --><p>Convenience functions can defined to access the different fields of the complex interface from a StarPU data handle after a <a class="el" href="group__API__Data__Management.html#gae6eb535cf9bf46a7ef9ad2d845c675a2">starpu_data_acquire()</a> call:</p>
<div class="fragment"><div class="line"><span class="keywordtype">double</span> *starpu_complex_get_real(<a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> handle)</div><div class="line">{</div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface *complex_interface =</div><div class="line">          (<span class="keyword">struct </span>starpu_complex_interface *) <a class="code" href="group__API__Data__Interfaces.html#ga357281162710186412327c17f4a63535">starpu_data_get_interface_on_node</a>(handle, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>);</div><div class="line">        <span class="keywordflow">return</span> complex_interface-&gt;real;</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keywordtype">double</span> *starpu_complex_get_imaginary(<a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> handle);</div><div class="line"><span class="keywordtype">int</span> starpu_complex_get_nx(<a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> handle);</div></div><!-- fragment --><p>Similar functions need to be defined to access the different fields of the complex interface from a <code>void *</code> pointer to be used within codelet implemetations.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#define STARPU_COMPLEX_GET_REAL(interface)      (((struct starpu_complex_interface *)(interface))-&gt;real)</span></div><div class="line"><span class="preprocessor">#define STARPU_COMPLEX_GET_IMAGINARY(interface) (((struct starpu_complex_interface *)(interface))-&gt;imaginary)</span></div><div class="line"><span class="preprocessor">#define STARPU_COMPLEX_GET_NX(interface)        (((struct starpu_complex_interface *)(interface))-&gt;nx)</span></div></div><!-- fragment --><p> Complex data interfaces can then be registered to StarPU.</p>
<div class="fragment"><div class="line"><span class="keywordtype">double</span> real = 45.0;</div><div class="line"><span class="keywordtype">double</span> imaginary = 12.0;</div><div class="line">starpu_complex_data_register(&amp;handle1, <a class="code" href="group__API__Codelet__And__Tasks.html#ga64855af2ea04f74a1a261724b3b79046">STARPU_MAIN_RAM</a>, &amp;real, &amp;imaginary, 1);</div><div class="line"><a class="code" href="group__API__Insert__Task.html#gad79a50a21fe717126659b2998209c1c6">starpu_task_insert</a>(&amp;cl_display, <a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dbaab55b4cef9cfedeacae7012cd52e5389">STARPU_R</a>, handle1, 0);</div></div><!-- fragment --><p>and used by codelets.</p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> display_complex_codelet(<span class="keywordtype">void</span> *descr[], <span class="keywordtype">void</span> *_args)</div><div class="line">{</div><div class="line">        <span class="keywordtype">int</span> nx = STARPU_COMPLEX_GET_NX(descr[0]);</div><div class="line">        <span class="keywordtype">double</span> *real = STARPU_COMPLEX_GET_REAL(descr[0]);</div><div class="line">        <span class="keywordtype">double</span> *imaginary = STARPU_COMPLEX_GET_IMAGINARY(descr[0]);</div><div class="line">        <span class="keywordtype">int</span> i;</div><div class="line"></div><div class="line">        <span class="keywordflow">for</span>(i=0 ; i&lt;nx ; i++)</div><div class="line">        {</div><div class="line">                fprintf(stderr, <span class="stringliteral">&quot;Complex[%d] = %3.2f + %3.2f i\n&quot;</span>, i, real[i], imaginary[i]);</div><div class="line">        }</div><div class="line">}</div></div><!-- fragment --><p>The whole code for this complex data interface is available in the directory <code>examples/interface/</code>.</p>
<h2><a class="anchor" id="DefiningANewDataInterface_allocation"></a>
Data allocation</h2>
<p>To be able to run tasks on GPUs etc. StarPU needs to know how to allocate a buffer for the interface. In our example, two allocations are needed in the allocation complex_allocate_data_on_node() method: one for the real part and one for the imaginary part.</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> starpu_ssize_t complex_allocate_data_on_node(<span class="keywordtype">void</span> *data_interface, <span class="keywordtype">unsigned</span> node)</div><div class="line">{</div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface *complex_interface = (<span class="keyword">struct </span>starpu_complex_interface *) data_interface;</div><div class="line"></div><div class="line">        <span class="keywordtype">double</span> *addr_real = NULL;</div><div class="line">        <span class="keywordtype">double</span> *addr_imaginary = NULL;</div><div class="line">        starpu_ssize_t requested_memory = complex_interface-&gt;nx * <span class="keyword">sizeof</span>(complex_interface-&gt;real[0]);</div><div class="line"></div><div class="line">        addr_real = (<span class="keywordtype">double</span>*) <a class="code" href="group__API__Data__Interfaces.html#gab91cbc596a65e6a4322b657c79934269">starpu_malloc_on_node</a>(node, requested_memory);</div><div class="line">        <span class="keywordflow">if</span> (!addr_real)</div><div class="line">                <span class="keywordflow">goto</span> fail_real;</div><div class="line">        addr_imaginary = (<span class="keywordtype">double</span>*) <a class="code" href="group__API__Data__Interfaces.html#gab91cbc596a65e6a4322b657c79934269">starpu_malloc_on_node</a>(node, requested_memory);</div><div class="line">        <span class="keywordflow">if</span> (!addr_imaginary)</div><div class="line">                <span class="keywordflow">goto</span> fail_imaginary;</div><div class="line"></div><div class="line">        <span class="comment">/* update the data properly in consequence */</span></div><div class="line">        complex_interface-&gt;real = addr_real;</div><div class="line">        complex_interface-&gt;imaginary = addr_imaginary;</div><div class="line"></div><div class="line">        <span class="keywordflow">return</span> 2*requested_memory;</div><div class="line"></div><div class="line">fail_imaginary:</div><div class="line">        <a class="code" href="group__API__Data__Interfaces.html#ga7e3ef9efbc7a65adad27f9ac27493493">starpu_free_on_node</a>(node, (uintptr_t) addr_real, requested_memory);</div><div class="line">fail_real:</div><div class="line">        <span class="keywordflow">return</span> -ENOMEM;</div><div class="line">}</div></div><!-- fragment --><p>Here we try to allocate the two parts. If either of them fails, we return -ENOMEM. If they succeed, we can record the obtained pointers and returned the amount of allocated memory (for memory usage accounting).</p>
<p>Conversely, complex_free_data_on_node() frees the two parts:</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">void</span> complex_free_data_on_node(<span class="keywordtype">void</span> *data_interface, <span class="keywordtype">unsigned</span> node)</div><div class="line">{</div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface *complex_interface = (<span class="keyword">struct </span>starpu_complex_interface *) data_interface;</div><div class="line">        starpu_ssize_t requested_memory = complex_interface-&gt;nx * <span class="keyword">sizeof</span>(complex_interface-&gt;real[0]);</div><div class="line"></div><div class="line">        <a class="code" href="group__API__Data__Interfaces.html#ga7e3ef9efbc7a65adad27f9ac27493493">starpu_free_on_node</a>(node, (uintptr_t) complex_interface-&gt;real, requested_memory);</div><div class="line">        <a class="code" href="group__API__Data__Interfaces.html#ga7e3ef9efbc7a65adad27f9ac27493493">starpu_free_on_node</a>(node, (uintptr_t) complex_interface-&gt;imaginary, requested_memory);</div><div class="line">}</div></div><!-- fragment --><p>We we have not made anything particular for GPUs or whatsoever: it is <a class="el" href="group__API__Data__Interfaces.html#ga7e3ef9efbc7a65adad27f9ac27493493">starpu_free_on_node()</a> which knows how to actually make the allocation, and returns the resulting pointer, be it in main memory, in GPU memory, etc.</p>
<h2><a class="anchor" id="DefiningANewDataInterface_copy"></a>
Data copy</h2>
<p>Now that StarPU knows how to allocate/free a buffer, it needs to be able to copy over data into/from it. Defining a copy_any_to_any method allows StarPU to perform direct transfers between main memory and GPU memory.</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> copy_any_to_any(<span class="keywordtype">void</span> *src_interface, <span class="keywordtype">unsigned</span> src_node,</div><div class="line">                           <span class="keywordtype">void</span> *dst_interface, <span class="keywordtype">unsigned</span> dst_node,</div><div class="line">                           <span class="keywordtype">void</span> *async_data)</div><div class="line">{</div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface *src_complex = src_interface;</div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface *dst_complex = dst_interface;</div><div class="line">        <span class="keywordtype">int</span> ret = 0;</div><div class="line"></div><div class="line"></div><div class="line">        <span class="keywordflow">if</span> (<a class="code" href="group__API__Data__Interfaces.html#ga7af8e4f90b557fefaa09f40a930efd74">starpu_interface_copy</a>((uintptr_t) src_complex-&gt;real, 0, src_node,</div><div class="line">                                    (uintptr_t) dst_complex-&gt;real, 0, dst_node,</div><div class="line">                                     src_complex-&gt;nx*<span class="keyword">sizeof</span>(src_complex-&gt;real[0]),</div><div class="line">                                     async_data))</div><div class="line">                ret = -EAGAIN;</div><div class="line">        <span class="keywordflow">if</span> (<a class="code" href="group__API__Data__Interfaces.html#ga7af8e4f90b557fefaa09f40a930efd74">starpu_interface_copy</a>((uintptr_t) src_complex-&gt;imaginary, 0, src_node,</div><div class="line">                                    (uintptr_t) dst_complex-&gt;imaginary, 0, dst_node,</div><div class="line">                                     src_complex-&gt;nx*<span class="keyword">sizeof</span>(src_complex-&gt;imaginary[0]),</div><div class="line">                                     async_data))</div><div class="line">                ret = -EAGAIN;</div><div class="line">        <span class="keywordflow">return</span> ret;</div><div class="line">}</div></div><!-- fragment --><p>We here again have no idea what is main memory or GPU memory, or even if the copy is synchronous or asynchronous: we just call <a class="el" href="group__API__Data__Interfaces.html#ga7af8e4f90b557fefaa09f40a930efd74">starpu_interface_copy()</a> according to the interface, passing it the pointers, and checking whether it returned -EAGAIN, which means the copy is asynchronous, and StarPU will appropriately wait for it thanks to the <code>async_data</code> pointer.</p>
<p>This copy method is referenced in a <a class="el" href="group__API__Data__Interfaces.html#structstarpu__data__copy__methods">starpu_data_copy_methods</a> structure:</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">struct </span><a class="code" href="group__API__Data__Interfaces.html#structstarpu__data__copy__methods">starpu_data_copy_methods</a> complex_copy_methods =</div><div class="line">{</div><div class="line">        .<a class="code" href="group__API__Data__Interfaces.html#add029fa5a3dc52c964587d97aca34263">any_to_any</a> = copy_any_to_any</div><div class="line">};</div></div><!-- fragment --><p>which was referenced in the <a class="el" href="group__API__Data__Interfaces.html#structstarpu__data__interface__ops">starpu_data_interface_ops</a> structure above.</p>
<p>Other fields of <a class="el" href="group__API__Data__Interfaces.html#structstarpu__data__copy__methods">starpu_data_copy_methods</a> allow to provide optimized variants, notably for the case of 2D or 3D matrix tiles with non-trivial ld.</p>
<h2><a class="anchor" id="DefiningANewDataInterface_pack"></a>
Data pack/unpack</h2>
<p>The copy methods allow for RAM/GPU transfers, but is not enough for e.g. transferring over MPI. That requires defining the pack/unpack methods. The principle is that the <a class="el" href="group__API__Data__Interfaces.html#a5fed1490798c49a6f26c91f1bc4da360">starpu_data_interface_ops::pack_data</a> method concatenates the buffer data into a newly-allocated contiguous bytes array, conversely <a class="el" href="group__API__Data__Interfaces.html#ab997f2654cd769dbd3413f5a90090a69">starpu_data_interface_ops::unpack_data</a> extracts from a bytes array into the buffer data and frees the bytes array.</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> complex_pack_data(<a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> handle, <span class="keywordtype">unsigned</span> node, <span class="keywordtype">void</span> **ptr, starpu_ssize_t *count)</div><div class="line">{</div><div class="line">        <a class="code" href="group__API__Toolbox.html#ga05ac0dda104331f57d85823a4f9318ce">STARPU_ASSERT</a>(starpu_data_test_if_allocated_on_node(handle, node));</div><div class="line"></div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface *complex_interface = (<span class="keyword">struct </span>starpu_complex_interface *)</div><div class="line">                <a class="code" href="group__API__Data__Interfaces.html#ga357281162710186412327c17f4a63535">starpu_data_get_interface_on_node</a>(handle, node);</div><div class="line"></div><div class="line">         *count = complex_get_size(handle);</div><div class="line">        <span class="keywordflow">if</span> (ptr != NULL)</div><div class="line">        {</div><div class="line">                <span class="keywordtype">char</span> *data;</div><div class="line">                data = (<span class="keywordtype">void</span>*) <a class="code" href="group__API__Data__Interfaces.html#gab2bf7713cad5570775bdf4efec79502d">starpu_malloc_on_node_flags</a>(node, *count, 0);</div><div class="line">                 *ptr = data;</div><div class="line">                memcpy(data, complex_interface-&gt;real, complex_interface-&gt;nx*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>));</div><div class="line">                memcpy(data+complex_interface-&gt;nx*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>), complex_interface-&gt;imaginary, complex_interface-&gt;nx*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>));</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p>complex_pack_data() first computes the size to be allocated, then allocates it, and copies over into it the content of the two real and imaginary arrays.</p>
<div class="fragment"><div class="line"><span class="keyword">static</span> <span class="keywordtype">int</span> complex_unpack_data(<a class="code" href="group__API__Data__Management.html#gad6bed33cdb76ef504efcdf05e5788076">starpu_data_handle_t</a> handle, <span class="keywordtype">unsigned</span> node, <span class="keywordtype">void</span> *ptr, <span class="keywordtype">size_t</span> count)</div><div class="line">{</div><div class="line">        <span class="keywordtype">char</span> *data = ptr;</div><div class="line">        <a class="code" href="group__API__Toolbox.html#ga05ac0dda104331f57d85823a4f9318ce">STARPU_ASSERT</a>(starpu_data_test_if_allocated_on_node(handle, node));</div><div class="line"></div><div class="line">        <span class="keyword">struct </span>starpu_complex_interface *complex_interface = (<span class="keyword">struct </span>starpu_complex_interface *)</div><div class="line">                <a class="code" href="group__API__Data__Interfaces.html#ga357281162710186412327c17f4a63535">starpu_data_get_interface_on_node</a>(handle, node);</div><div class="line"></div><div class="line">        <a class="code" href="group__API__Toolbox.html#ga05ac0dda104331f57d85823a4f9318ce">STARPU_ASSERT</a>(count == 2 * complex_interface-&gt;nx * <span class="keyword">sizeof</span>(<span class="keywordtype">double</span>));</div><div class="line">        memcpy(complex_interface-&gt;real, data, complex_interface-&gt;nx*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>));</div><div class="line">        memcpy(complex_interface-&gt;imaginary, data+complex_interface-&gt;nx*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>), complex_interface-&gt;nx*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>));</div><div class="line"></div><div class="line">        <a class="code" href="group__API__Data__Interfaces.html#ga02005aa2a3c838802d95d0426a937d8d">starpu_free_on_node_flags</a>(node, (uintptr_t) ptr, count, 0);</div><div class="line"></div><div class="line">        <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p>complex_unpack_data() simply uses memcpy to copy over from the bytes array into the data buffer, and releases the bytes array.</p>
<h1><a class="anchor" id="SpecifyingATargetNode"></a>
Specifying A Target Node For Task Data</h1>
<p>When executing a task on a GPU for instance, StarPU would normally copy all the needed data for the tasks on the embedded memory of the GPU. It may however happen that the task kernel would rather have some of the datas kept in the main memory instead of copied in the GPU, a pivoting vector for instance. This can be achieved by setting the <a class="el" href="group__API__Codelet__And__Tasks.html#a018dfc2c8999ad7646d37ac6c60a5c9e">starpu_codelet::specific_nodes</a> flag to <code>1</code>, and then fill the <a class="el" href="group__API__Codelet__And__Tasks.html#ab801e0bab4e8a5feafa30d29335e7c9e">starpu_codelet::nodes</a> array (or <a class="el" href="group__API__Codelet__And__Tasks.html#a433f15f25d2dc79763a154a6233f75f7">starpu_codelet::dyn_nodes</a> when <a class="el" href="group__API__Codelet__And__Tasks.html#a1bb02f890f2e10c348499dbd92b56496">starpu_codelet::nbuffers</a> is greater than <a class="el" href="group__API__Codelet__And__Tasks.html#gad9efd8b217907a2fe26d92bd91438cdf">STARPU_NMAXBUFS</a>) with the node numbers where data should be copied to, or <a class="el" href="group__API__Codelet__And__Tasks.html#gaf9635620e7d808cd9bde3e6278dcc326">STARPU_SPECIFIC_NODE_LOCAL</a> to let StarPU copy it to the memory node where the task will be executed.</p>
<p>::STARPU_SPECIFIC_NODE_CPU can also be used to request data to be put in CPU-accessible memory (and let StarPU choose the NUMA node). ::STARPU_SPECIFIC_NODE_FAST and ::STARPU_SPECIFIC_NODE_SLOW can also be used</p>
<p>For instance, with the following codelet:</p>
<div class="fragment"><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Codelet__And__Tasks.html#structstarpu__codelet">starpu_codelet</a> cl =</div><div class="line">{</div><div class="line">        .<a class="code" href="group__API__Codelet__And__Tasks.html#aa6a8436117176270c5372d4dfb006a1f">cuda_funcs</a> = { kernel },</div><div class="line">        .nbuffers = 2,</div><div class="line">        .modes = {<a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba9fb7d314ccc154aead02dab90f8db52b">STARPU_RW</a>, STARPU_RW},</div><div class="line">        .specific_nodes = 1,</div><div class="line">        .nodes = {STARPU_SPECIFIC_NODE_CPU, <a class="code" href="group__API__Codelet__And__Tasks.html#gaf9635620e7d808cd9bde3e6278dcc326">STARPU_SPECIFIC_NODE_LOCAL</a>},</div><div class="line">};</div></div><!-- fragment --><p>the first data of the task will be kept in the CPU memory, while the second data will be copied to the CUDA GPU as usual. A working example is available in <code>tests/datawizard/specific_node.c</code></p>
<p>With the following codelet:</p>
<div class="fragment"><div class="line"><span class="keyword">struct </span><a class="code" href="group__API__Codelet__And__Tasks.html#structstarpu__codelet">starpu_codelet</a> cl =</div><div class="line">{</div><div class="line">        .<a class="code" href="group__API__Codelet__And__Tasks.html#aa6a8436117176270c5372d4dfb006a1f">cuda_funcs</a> = { kernel },</div><div class="line">        .nbuffers = 2,</div><div class="line">        .modes = {<a class="code" href="group__API__Data__Management.html#gga1fb3a1ff8622747d653d1b5f41bc41dba9fb7d314ccc154aead02dab90f8db52b">STARPU_RW</a>, STARPU_RW},</div><div class="line">        .specific_nodes = 1,</div><div class="line">        .nodes = {<a class="code" href="group__API__Codelet__And__Tasks.html#gaf9635620e7d808cd9bde3e6278dcc326">STARPU_SPECIFIC_NODE_LOCAL</a>, STARPU_SPECIFIC_NODE_SLOW},</div><div class="line">};</div></div><!-- fragment --><p>The first data will be copied into fast (but probably size-limited) local memory while the second data will be left in slow (but large) memory. This makes sense when the kernel does not make so many accesses to the second data, and thus data being remote e.g. over a PCI bus is not a performance problem, and avoids filling the fast local memory with data which does not need the performance. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Fri Oct 22 2021 15:02:13 for StarPU Handbook by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
